{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models creation tutorial for Debiai example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System modules\n",
    "import importlib\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "# Tensorflow modules\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Models and results functions\n",
    "\n",
    "#### Here are some functions that will be helpful during the example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_from_dir(path, batch_size=32, nb_layers=3):\n",
    "    \"\"\" \n",
    "    Create a CNN model from directories of images grouped by labels.\n",
    "    Return the train and val dataset and the model.\n",
    "    \"\"\"\n",
    "    data_dir = pathlib.Path(path)\n",
    "    \n",
    "    # Create a dataset\n",
    "    img_height = 32\n",
    "    img_width = 32\n",
    "\n",
    "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset='training',\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "\n",
    "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "        data_dir,\n",
    "        validation_split=0.2,\n",
    "        subset='validation',\n",
    "        seed=123,\n",
    "        image_size=(img_height, img_width),\n",
    "        batch_size=batch_size)\n",
    "    \n",
    "    model = create_cnn_model(batch_size=batch_size, nb_layers=nb_layers)\n",
    "    \n",
    "    return (train_ds, val_ds, model)\n",
    " \n",
    "def create_cnn_model(batch_size=32, nb_layers=3):\n",
    "    \"\"\" Return a CNN model for 32*32*3 inputs images \n",
    "        nb_layers allow to choose number of Conv2D, MaxPooling2D layers\n",
    "    \"\"\"\n",
    "    # Create model\n",
    "    num_classes = 10\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    \n",
    "    \n",
    "    model.add(layers.experimental.preprocessing.Rescaling(1./255))\n",
    "    \n",
    "    for i in range(nb_layers):\n",
    "        model.add(layers.Conv2D(32, 3, activation='relu'))\n",
    "        model.add(layers.MaxPooling2D())\n",
    "        \n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dense(num_classes))\n",
    "\n",
    "    # Compile model functions\n",
    "    model.compile(\n",
    "      optimizer='adam',\n",
    "      loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      metrics=['accuracy'])\n",
    "\n",
    "    model.build((batch_size,32,32,3))\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The first model will also create the train and validation dataset for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 70000 files belonging to 10 classes.\n",
      "Using 56000 files for training.\n",
      "Found 70000 files belonging to 10 classes.\n",
      "Using 14000 files for validation.\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rescaling (Rescaling)        (32, 32, 32, 3)           0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (32, 30, 30, 32)          896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (32, 15, 15, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (32, 7200)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (32, 128)                 921728    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (32, 10)                  1290      \n",
      "=================================================================\n",
      "Total params: 923,914\n",
      "Trainable params: 923,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rescaling_1 (Rescaling)      (32, 32, 32, 3)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (32, 30, 30, 32)          896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (32, 15, 15, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (32, 13, 13, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (32, 6, 6, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (32, 4, 4, 32)            9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (32, 2, 2, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (32, 128)                 0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (32, 128)                 16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (32, 10)                  1290      \n",
      "=================================================================\n",
      "Total params: 37,194\n",
      "Trainable params: 37,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Found 67085 files belonging to 10 classes.\n",
      "Using 53668 files for training.\n",
      "Found 67085 files belonging to 10 classes.\n",
      "Using 13417 files for validation.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rescaling_2 (Rescaling)      (32, 32, 32, 3)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (32, 30, 30, 32)          896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (32, 15, 15, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (32, 13, 13, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (32, 6, 6, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (32, 4, 4, 32)            9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (32, 2, 2, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (32, 128)                 0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (32, 128)                 16512     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (32, 10)                  1290      \n",
      "=================================================================\n",
      "Total params: 37,194\n",
      "Trainable params: 37,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "rescaling_3 (Rescaling)      (32, 32, 32, 3)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (32, 30, 30, 32)          896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (32, 15, 15, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (32, 13, 13, 32)          9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (32, 6, 6, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (32, 4, 4, 32)            9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (32, 2, 2, 32)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (32, 128)                 0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (32, 128)                 16512     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (32, 10)                  1290      \n",
      "=================================================================\n",
      "Total params: 37,194\n",
      "Trainable params: 37,194\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# We get the mnist dataset, validation set and model from this function\n",
    "(mnist_ds, mnist_val, mnist_model_1) = create_model_from_dir(\"data/MNIST_reformat/\", nb_layers=1)\n",
    "\n",
    "# We want to try another model with more Conv2D layers\n",
    "mnist_model_2 = create_cnn_model(nb_layers=3)\n",
    "\n",
    "# We create datasets and model for MNIST_M\n",
    "(mnistm_ds, mnistm_val, mnistm_model_1) = create_model_from_dir(\"data/MNIST_M/train\", nb_layers=3)\n",
    "\n",
    "# Last we can add a model trained on both dataset\n",
    "# Merge both dataset\n",
    "full_dataset = mnist_ds.concatenate(mnistm_ds)\n",
    "full_dataset.shuffle(1)\n",
    "full_val = mnist_val.concatenate(mnistm_val)\n",
    "\n",
    "full_model = create_cnn_model(nb_layers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1750/1750 [==============================] - 961s 549ms/step - loss: 0.1379 - accuracy: 0.9580 - val_loss: 0.0675 - val_accuracy: 0.9788\n",
      "1750/1750 [==============================] - 447s 256ms/step - loss: 0.1691 - accuracy: 0.9488 - val_loss: 0.0606 - val_accuracy: 0.9804\n",
      "1678/1678 [==============================] - 879s 524ms/step - loss: 0.6127 - accuracy: 0.8037 - val_loss: 0.2854 - val_accuracy: 0.9135\n",
      "3428/3428 [==============================] - 974s 284ms/step - loss: 0.2730 - accuracy: 0.9177 - val_loss: 0.1494 - val_accuracy: 0.9540\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbbf09e2400>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model with mnist (1 layer)\n",
    "mnist_model_1.fit(mnist_ds, validation_data=mnist_val, epochs=1)\n",
    "\n",
    "# Train the model with mnist (3 layers)\n",
    "mnist_model_2.fit(mnist_ds, validation_data=mnist_val, epochs=1)\n",
    "\n",
    "# Train the model with mnist_m (3 layers)\n",
    "mnistm_model_1.fit(mnistm_ds, validation_data=mnistm_val, epochs=1)\n",
    "\n",
    "# Train the model with mnist and mnist_m (3 layers)\n",
    "full_model.fit(full_dataset,validation_data=full_val,epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/viri0x/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/viri0x/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: models/mnist_model_1/assets\n",
      "INFO:tensorflow:Assets written to: models/mnist_model_2/assets\n",
      "INFO:tensorflow:Assets written to: models/mnistm_model_1/assets\n",
      "INFO:tensorflow:Assets written to: models/full_model/assets\n"
     ]
    }
   ],
   "source": [
    "# Save all models in models/ directory\n",
    "mnist_model_1.save(\"models/mnist_model_1\")\n",
    "mnist_model_2.save(\"models/mnist_model_2\")\n",
    "mnistm_model_1.save(\"models/mnistm_model_1\")\n",
    "full_model.save(\"models/full_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CONCLUSION\n",
    "\n",
    "#### You now have 4 models that will be load and used in the next tutorial which is about how to push data and results into debiai from the package."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
